## CLIP

### 定义： 

​		CLIP(Contrastive Language–Image Pretraining)，也称为对比学习，这是一种VLP(Vision-Language Pretraining)多模态预训练模型。由OpenAI在2021年提出。此模型使用了从互联网上搜索的大约4亿个图像-文本对作为Pretraining Dataset（预训练数据），Pretraining task（预训练任务）选择的是Image-Text Contrastive Learning（图像文本对比学习），旨在将图像和文本的特征表示对齐到共享的特征空间中，以此达成即使在零样本或少样本的情况下也能进行高效的跨模态推理的效果。

### 原理：

<img src=".\img\image-20240110160810685.png" alt="image-20240110160810685"  />



#### 预训练阶段：

​		首先给定N个image-text文本图像对。在我们训练的时候每个图像会和每个文本形成一个文本图像对，也就是说有NxN个文本图像对，其中有N个正确匹配的图像对（对角线），有N<sup>2</sup>-N个错误匹配的图像对。每一个图像文本对通过计算余弦相似度来表示图像与文本之间的相似度。而我们的目的就是最大化正确匹配的图像文本对的相似度，最小化错误匹配的图像文本对的相似度，以此让图像与其对应文本在特征空间中更加靠近，增加模型匹配正确文本图片的性能。

​		余弦相似度计算式如下：	 
                                                                   <img src=".\img\image-20240110181130501.png" alt="image-20240110181130501.png"  />

​		提升正确匹配的图像文本对相似度通过损失函数来实现。

​		CLIP的损失函数如下：
<img src=".\img\image-20240110162141323.png" alt="image-20240110162141323.png"  />


​		其中

<img src=".\img\image-20240110181317125.png" alt="image-20240110181317125.png"  />

​																												**x表示图像，t表示文本**

​		L<sub>i2t</sub>表示的是从图片到文本的损失，针对每一个图像，我们应用softmax cross-entropy函数。在softmax中，分子是这张图片与匹配文本的相似度的指数表示，分母是这张图片与所有文本的相似度的指数的和。最终的L<sub>i2t</sub>会将针对每一个图像的softmax cross-entropy加和。

​		L<sub>t2i</sub>同理，只不过每一次针对的是一个文本和所有图像。

​		最终L<sub>CLIP</sub>就是L<sub>i2t</sub>与L<sub>t2i</sub>的和。如果倒退的话可以发现，L<sub>CLIP</sub>越小，L<sub>i2t</sub>与L<sub>t2i</sub>就会越小，由于是-log函数，就会让整个softmax函数变大，也就让分子变大，而分子也就是指的正确匹配的图像文本对的相似度。

#### 预测阶段

​		预测阶段如右图所示。此图再做一个零样本分类任务，首先给定类别标签（这些类别CLIP可能从来都没见过），然后通过一个提示模版（prompt template）为每个类别生成句子（这是因为在训练过程中模型见到的通常都是句子，并且如果不是一个句子的话，会导致没有上下文而产生歧义，比如在Oxford-IIIT Pet数据集中boxer指的是一条小狗，而如果没有上下文的话大概率会任务boxer是拳击手）。

​		之后这一系列生成的句子会和这张图片分别计算相似度，其中和这张图片相似度最大的那个文本中的类别也就代表了这张图片的类别。

