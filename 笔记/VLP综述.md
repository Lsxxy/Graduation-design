# **VLP模型综述**

## **一.VLP模型的历史**

1.**Small-scale task-specific method design (2014/11-2019/8)**：这段时间的技术主要针对特定任务进行开发。

像针对image captioning和VQA出现的技术主要有：使用ResNet，Faster RCNN这种深度学习框架提取视觉特征；基于预训练词嵌入的方法，它将词语或短语转换为长度固定的向量；针对序列数据的LSTM模型；注意力机制等。

**2.Medium-scale pre-training (2019/8-2021/8)**: 这段时间受到BERT在NLP领域的巨大成功，VL模型逐渐转向基于使用Transformer的多模态融合模型，并且开始在中等规模的数据集上进行预训练，比如使用大约10M的图像-文本对来训练，比如UNITER模型以及OSCAR模型。

**3.Large-scale pre-training (2021/8-now):** 随着CLIP (Radford et al., 2021) 和 ALIGN (Jia et al., 2021) 的出现，大规模的视觉语言预训练（VLP）显示出巨大的潜力，并成为视觉-语言（VL）研究的基础。VLP的高计算成本可以通过将预训练模型适配到广泛的下游任务中来分摊。这时候数据集已经可以达到12B，模型参数达到5B。

## **二.VLP模型所面临的代表性Task**

VLP模型所面临的任务大致可以分为三类，Image-Text Tasks（图像-文本任务），Computer Vision Tasks（计算机视觉任务，老任务，迁移过去），Video-Text Tasks（视频-文本任务）

## **(一）Image-Text Tasks**

### **1.Image-text Retrieval**

**(1)类别**：一般分为两个子任务，一个是给定图像，检索相关文本的image-to-text retrieval；另一个是给定文本，检索相关图像的text-to-image retrieval。

**(2)评估指标**：Recall@K (K=1, 5, 10)

Recall@K的定义是：在前K个检索结果中，有多少比例的相关项被模型检索到。

- •

  **Recall@1** 指的是模型在其返回的第一个结果中检索到相关项的概率。

- •

  **Recall@5** 指的是模型在其返回的前五个结果中检索到相关项的概率。

- •

  **Recall@10** 指的是模型在其返回的前十个结果中检索到相关项的概率。

**(3)数据集**：COCO，Flickr30K

### **2.Visual Question Answering and Visual Reasoning**

**(1)两个任务的定义联系于区别**

### **VQA任务**

**定义：**

- •

  VQA任务涉及理解给定的图像并对与该图像相关的自然语言问题给出回答。这要求模型能够识别图像中的对象、活动、场景等元素，并理解问题中的语言信息。

**特点：**

- •

  VQA任务通常集中于直接的图像内容理解，如识别、分类或描述图像中的物体。

- •

  问题可能涉及图像的具体细节，如“照片中的狗是什么颜色的？”或者对图像中的活动进行简单的描述。

### **Visual Reasoning任务**

**定义：**

- •

  Visual Reasoning任务要求更深层次的理解和推理，不仅仅是识别图像中的物体，而是理解物体之间的关系、图像中的动作或事件以及潜在的情境和上下文。

**特点：**

- •

  这类任务通常涉及更复杂的推理过程，比如理解空间关系、执行多步骤的逻辑推理或对图像内容的深层含义进行推断。

- •

  例如，Visual Reasoning可能包括理解物体间的相互作用或对图像情境进行常识推理。

### 联系与区别

**联系：**

- •

  Visual Reasoning任务可以看作是VQA任务的一个子集或扩展，因为它们都涉及到解释图像内容和理解与图像相关的问题。

- •

  两者都要求模型具备视觉理解和语言处理的能力。

**区别：**

- •

  VQA任务的重点通常在于直接的图像内容识别和基础问题回答，而Visual Reasoning任务则要求更高级的认知处理，如推理、判断和解释。

- •

  Visual Reasoning任务通常更具挑战性，需要对图像中的细微差别、复杂情境和隐含的逻辑关系有深入的理解。

**(2)评估指标：**

![4e3c13bd2781fdff4becea5b97274c8.png](https://s.readpaper.com/T/2X7mhRWYHQX)

也就是说当至少有三个人类给出和机器一样的答案时，得分就会较高，说明有较高的“人类一致性”。

### **3.Image Captioning**

**(1)定义:** Image Captioning 是为给定图像生成自由形式的文本标题。

**(2)评估指标:**

**基于n-gram（词的组合）重叠来评估生成的文本与参考文本（通常是人类编写的描述）之间的相似度**:

什么是n-gram:比如说生成的句子是“我喜欢猫”，而参考文本是“我喜欢狗”，现在我们使用2-gram，首先会将生成文本和参考文本以两个单词为单位进行分割，生成句子会被分割成“我喜”，“喜欢”，“欢猫”，而参考文本会生成“我喜”，“喜欢”，“欢狗”，之后会根据不同的n-gram评价指标计算指标，例如如果使用BLEU评价指标，就会是相同的n-gram除以总的n-gram数量，在这里也就是2/3。

n-gram评价指标有：BLEU，METEOR，ROUGE-L，CLDEr

**基于语义的评价指标。这种度量方法专注于评估生成的文本在语义内容上与参考文本的一致性。**

从上边的例子也可以看出，“我喜欢猫”和“我喜欢狗”明明有很大区别，但最后相似度却很高，这就是n-gram的缺点，没有考虑语义，所以有了这种基于语义的评估指标。

语义评价指标：SPICE，它通过分析文本中的命题（propositions）— 如主体、谓语和宾语 — 来衡量语义相似度。

**(3)数据集：**

single-sentence captions: COCO, TextCaps, No-Caps

multi-sentence captions: 少有人制作复杂数据集。

## **(二）Computer Vision Tasks**

计算机视觉任务就是一些比较经典的任务，比如图像分类，语义分割等。

**(1)Image Classification(图像分类）**

- •

  **目标：** 识别图像中的主要对象或场景。

- •

  **优势：** VLP模型通过图像描述或相关文本标签来增强分类。

**(2)Object Detection(目标检测)：**

- •

  **目标：** 在图像中识别和定位多个对象。

- •

  **优势：** VLP模型可以通过理解与对象相关的文本信息来改善检测。

**(3)Semantic Segmentation（语义分割）：**

- •

  **目标：** 对图像中的每个像素进行分类，用于分割图像中的不同区域或对象。

- •

  **优势：** VLP模型可以通过文本提示来识别和分割特定类别的对象。

## **三.Task-specific VL Models**

这段时间的模型通常由以下部分组成：图像编码器，文本编码器，多模态融合模块，针对特定任务的输出层。

### **1.Visual Encoder**: 这段时间主要就是两个种类模型作为图像编码器。

**(1)CNN:** 由于当时使用CNN进行图像分类十分成功，所以当时的VL任务使用在ImageNet上进行预训练过的CNN网络（如VGGNet，ResNet等）作为Visual Encoder，提取最后一层特征作为全局图像特征。之后为了保留空间信息，开始使用模型前几层所产生的特征，称为grid features。

**(2)OD:** OD指的是Object Detection（目标检测），它可以检测图片中所有物品并利用方框进行定位，比较典型的例子是Faster RCNN，当我们使用OD作为Visual Encoder的时候指的是使用OD进行物体检测，并针对每个物体生成框，之后针对每个框提取物体图像特征（也称为区域特征 region feature），最后根据所问问题来找到具体的物体框的特征来回答问题。

**2.Text Encoder:** Text Encoder会将我们输入的句子先分词，再进行词嵌入，将每个词转换为词向量，之后应用模型进行文本编码，早期的文本编码模型有三类。

**(1)BoW(Bag-of-Words):** 在Bow中，每个单词被独立编码，通常不考虑它们之间的顺序或语境关系。这些编码后的单词向量可能会被求和或取平均来生成问题的最终表示。

**(2)RNN:** 也就是我们所说的循环神经网络，除了单词的编码之外，还考虑了单词之间的顺序，充分考虑了上下文信息。

**(3)Transformer:** 由于BERT的出现，Transformer模块被广泛用于提取文本特征，这种方法使用注意力机制，不仅能够捕捉足够长的上下文关系，同时提供了高效率的并行计算能力。

###### **3.Multimodal Fusion Module：** 多模态融合模块，旨在建立视觉特征和文本特征之间的交互作用，以更好地理解和处理包含图像和文本的数据。

### **(1)简单融合**

- •

  **方法：** 图像和文本特征通过元素级乘积、求和或连接进行融合。

- •

  **进一步细化：** 一些更复杂的设计通过使用LSTM或多模态残差网络来细化融合后的图像-文本特征。

### **(2)跨模态注意力(Inter-modality attention)**

- •

  **目的：** 捕捉图像和文本输入之间的多模态对齐。

- •

  **方法：**

  - •

    **叠加注意力网络（SAN）：** 在VQA中首次验证了跨模态注意力的有效性，使用问题作为查询来关注图像特征。

  - •

    **共注意力方法：** 同时进行问题引导的图像注意力和图像引导的文本注意力。

  - •

    **双线性注意力（BAN）：** 考虑问题中每个单词和图像区域之间的每一对组合。

  - •

    **多层注意力：** 可以视为多步推理过程，注意力分布逐层细化，专注于与问题更相关的区域。

### **(3)内部模态注意力(Intra-modality attention)**

- •

  **目的：** 在图像区域或问题单词之间进行关系推理,主要针对单模态，帮助模型识别图像或文本最重要的地方。虽然他是但模态，但这种方法结合跨模态模块可以达到更好的效果。

- •

  **方法：** 通过构建图结构表示（例如，使用依赖解析构建的问题中的单词图，或利用外部知识和基于规则的先验构建的图像中的对象区域图）来提高VQA性能。

### **(4)Transformer**

- •

  **特点：** 不仅通过跨模态注意力关注其他模态，还通过内部模态注意力关注当前模态中的相关区域或单词。(跨模态+内模态）

- •

  **应用：** MCAN模型使用Transformer架构，其中编码器使用自注意力单元进行内部模态交互（区域到区域或单词到单词），解码器则使用引导注意力单元进行密集的跨模态交互。

### **4.Task-specific Output Layer**：针对不同的任务要使用不同的预测层

比如 VQA 通常被建模为分类问题，输出层是一个由全连接层或多层感知器和 softmax 层组成的分类器来预测答案。

## **四.VLP Models**

受到语言模型预训练的成功以及语言模型和视觉模型开始统一都使用Transformer，VLP模型开始流行。(在第一部分说的**Medium-scale pre-training**和**Large-scale pre-training**都属于VLP领域）

## **1.模型分类**

VLP模型大概可以分为两类，按照如何融合视觉与文本特征分为dual encoder和fusion encoder。

## **(一)fusion encoder**

fusion encoder是指除了使用image encoder和text encoder来提取文本和图像特征之外，还会额外使用一个Transformer layers来对文本特征和图像特征进行深层的融合。

fusion encoder可以分为两类，一类是两阶段训练的two-stage pre-training pipeline，另一类是端到端的end-toend pre-training。

## ***1.模型分类***

### **1.two-stage pre-training**

**(1)定义**： 这其中最为经典的模型就是OD-based VLP Models，会首先使用预训练好的目标检测模型（如Faster-RCNN）来提取区域特征，之后通过Transformer模块来提取融合文本和区域特征。

### **2.End-to-End VLP Models**

**(1)定义**：端到端的模型可以通过卷积神经网络（CNNs）、视觉Transformer（ViTs）或直接使用图像块嵌入来提取图像特征，而不需要像两阶段模型一样提前提取图像区域特征。

**(2)方法：**

**CNN-based Grid** 

- •

  **模型示例：** PixelBERT和CLIP-ViL。

- •

  **方法：** 这些模型直接将CNN提取的网格特征和文本送入一个Transformer模型。

- •

  **特点：** 使用网格特征可以提高效率，但通常需要为CNN和Transformer使用不同的优化器。例如，PixelBERT和CLIP-ViL分别为Transformer使用AdamW优化器，为CNN使用SGD优化器。

**ViT-based Patch Features**

- •

  **模型示例：** ViLT、ViTCAP、UFO、VLMo。

- •

  **方法：** 这些模型使用Transformer模型来提取图像特征。

## ***2.模型组成***

在fusion encoder中，模型架构可以分为三部分，视觉编码器，文本编码器，多模态融合器

### **1.Vision Encoder**

**(1)OD:** OD主要应用于two-stage pre-training(放入Transformer模块前提取区域特征），其中最为流行的是faster-rcnn。 之后为了加入位置信息，提高性能，VinVL提出了一种基于ResNeXt-152 C4架构的OD模型，并得到了显著的性能提升。

**(2)CNN**:VLP中使用的CNN提取更通用的图像特征，基本使用的都是Resnet模型及其变体。并且我们发现更强大的CNN模型能让网络在下游任务中有更好的表现。

- •

  PixelBERT、SOHO和CLIP-ViL等模型中，使用了如在ImageNet上预训练的ResNet-50、ResNet-101和ResNeXt-152来提取图像特征。

- •

  CLIP-ViL中，使用了从CLIP模型预训练的ResNet变体。

- •

  SimVLM使用了ResNet-101和ResNet-152的前三个块（排除了Conv stem）来构建其基础和大型模型，并使用了通道数更多的ResNet-152变体来构建巨型模型。

**(3)ViT:**  即Vision Transformer,首先将图像分割成图像块，然后将这些块展平成向量并线性投影以获得块嵌入。并将一个可学习的特殊token（[CLS]）嵌入也被添加到序列的开头，之后送入多层Transformer模块以获得最终的输出图像特征。

- •

  **ViT变体：** 原始的ViT、DeiT、BEiT、Swin Transformer和CLIP-ViT等。

### **2.Text Encoder**

**(1)使用模型：** 使用的模型大同小异，都是BERT或它的变体，如RoBERTa、ELECTRA、ALBERT、DeBERTa。

**(2)处理过程：** 先将句子进行分割，之后在句首和句尾加上特殊标记，生成文本序列，之后再放入模型中进行处理生成文本特征。

**(3)文本表示：** 无论使用哪种文本编码器，输入文本最终都被表示为一组特征向量集合。

### **3.Multimodal Fusion**

**(1)merged attention:** 在这种模块中，文本和视觉特征被简单地连接在一起，然后送入单个Transformer块。这种设计被用于VisualBERT、Unicoder-VL、VLP、VLBERT、UNITER、OSCAR、VinVL、ViLT、GIT等模型中。

**(2)co-attention:** 文本和视觉特征分别独立地送入不同的Transformer块，使用如交叉注意力等技术来实现跨模态交互。这种设计被用于LXMERT、ViLBERT、ERNIE-ViL、METER等模型中。

![image.png](https://s.readpaper.com/T/2XGAzkUDH7J)

**(3)对比：** merged attention的参数效率更高，对于end-to-end型的模型，使用Co-attention表现得更好，所以两个方法都各有优劣，需要根据具体模型选择。

## ***3.模型架构***

### **1. 仅编码器架构**

- •

  **应用：** 大多数VLP模型采用仅编码器架构。在这种架构中，跨模态表示直接输入到基于多层感知机（MLP）的输出层，以生成最终输出。

- •

  **适用任务：** 这种设计自然适合于视觉语言理解任务，如视觉问答（VQA）和视觉推理。

- •

  **用于图像描述：** 当用于图像描述生成时，同一个编码器充当解码器的角色，通过使用因果掩码（causal mask）逐个令牌地生成输出字幕。

### **2. 编码器-解码器架构**

- •

  **应用：** 在这些模型中，跨模态表示首先输入到解码器，然后再到输出层。解码器关注编码器的表示和之前生成的令牌，以自回归方式产生输出。

- •

  **优势：** 使用编码器-解码器架构可以实现对各种图像-文本任务的统一，以及VLP模型的零样本/小样本学习，并且自然适合于生成任务。

## ***4.Pre-training Task***

**定义：** 预训练任务是用于训练大型视觉语言模型之前的一系列任务，目的是在开始针对具体下游任务进行微调之前，先在大量未标注或部分标注的数据上学习通用的特征和表示。有着学习通用特征，改善下游任务表现的作用。

**(1)Masked Language Modeling(MLM):** 

- •

  **定义**：给定一个图像-文本对，随机遮盖输入文本中约15%的词，并用特殊的[MASK]标记替换这些被遮盖的词。目标是基于它们周围的词和配对的图像来预测这些掩码词。

- •

  **应用**：
          **Seq-MLM：** 被[Mask]的地方只能通过它之前的内容作为依据来预测。

  ​        **LM：** 模型从一个初始状态（可能是完全空白或包含几个启动词）开始，然后逐步生成后续的词汇，模型在生成每个新词时都依赖于之前已生成的所有词汇。

  ​        **Prefix-LM：** 把被[Mask]的句子分成两部分，带[Mask]的一般在后半部分，使用双向注意力处理“前缀”和图像，了解上下文信息。使用单向注意力预测[Mask]的词。

**(2)Image-Text Matching(ITM):**

- •

  **定义：** 在ITM任务中，模型需要判断给定的图像和文本（字幕）对是否相互匹配。

- •

  **流程：** 在输入句子的开始处添加一个特殊标记（如[CLS]），用于学习全局的跨模态表示。之后输入匹配或不匹配的图像-文本对〈v, w〉，其中v和w分别代表图像和文本的特征表示。之后在[CLS]标记上添加一个分类器，用来预测二元标签ys，表明所提供的图像-文本对是否匹配。

**(3)Image-Text Contrastive Learning(ITC):**

- •

  **定义：** 给定一个包含N个图像-文本对的批次，ITC的目标是从所有N^2个可能的图像-文本组合中预测出N个匹配对。

**(4)Masked Image Modeling(MIM)**

- •

  **定义：** 是在图像领域中对MLM的类比，模型需要根据剩余的可见图像区域或块和所有文本来重构被掩码的图像区域。

- •

  **分类：**

  **基于物体检测器（OD）的VLP模型中的MIM：**

  - •

    **操作方式：** 在这类模型（如LXMERT和UNITER）中，一些输入区域(region)被随机掩码（用零替换其视觉特征），模型通过最小化均方误差损失来重构原始区域特征。

   **端到端VLP模型中的MIM：**

  - •

    **掩码块回归/分类：** 在ViLT和METER等模型中，研究者探索了对掩码图像块进行回归或分类的MIM方法。

  - •

    **离散VQ标记的MIM：** 受BEiT启发，使用VQ-VAE模型将每个图像标记为一系列离散标记，模型训练以重构这些离散标记。

  - •

    **批内负样本的MIM：** 类似于MLM使用文本词汇表，模型通过动态构建的批内负样本词汇表来重构输入块

## ***5.Pre-training Datasets***

### **(1)学术环境下的预训练数据**集:

- •

  **常用数据集：** 在典型的学术环境中，VLP模型通常在四个常用的图像-字幕数据集上进行预训练，包括COCO、Visual Genome (VG)、Conceptual Captions (CC3M) 和 SBU Captions。

- •

  **数据集特点：**

  - •

    COCO和VG被视为“域内”数据集(In-domain datasets)，因为大多数视觉语言下游任务都是基于它们构建的。

  - •

    CC3M和SBU Captions被视为“域外”数据集(Out-of-domain datasets)。

  - •

    域内域外数据集主要看数据集中数据是否与模型将要执行的任务或应用场景非常相似

- •

  **其他数据集：** 最近，还有使用更大的数据集，如CC12M和Localized Narratives，后者的字幕更长，包含更丰富的描述。

### **(2)工业设置下的预训练数据集:**

- •

  **大规模网络抓取数据集：**

  - •

    CLIP使用的数据集包含4亿个图像-文本对，基于50万个查询构建。

  - •

    ALIGN使用的数据集有18亿个图像-文本对，数据收集管道类似于CC，但清理步骤更宽松。

  - •

    Wikipedia-based Image-Text Dataset (WIT) 包含1150万个独特图像和3760万个文本，特点是多语言。

  - •

    WenLan包含3000万个图像-文本对，经过精心清理。

  - •

    LAION-400M/5B包含4亿或50亿个图像-文本对，使用CLIP模型进行过滤。

  - •

    RedCaps包含1200万个图像-文本对，来自350个subreddits。

  - •

    Florence和GIT使用的数据集包含8亿个图像-文本对，包括ALT200M。

## ***6.Advanced Topics***

随着VLP的快速发展与完善，近些年出现了一些新的研究方向，这些方向都是**最新**的。

### **1.Big Models**

随着NLP领域的模型参数的迅速增长，VLP领域也展现出了相同的趋势，参数迅速增长。不同的Big Models采用不同的模型组件与架构，不同大小的数据集，不同的预训练任务，这些模型所擅长的下游任务与性能也有所不同。

![image.png](https://s.readpaper.com/T/2XNWah1WsQj)

**2.In-Context Few-Shot Learning**

In-Context Few-Shot Learning就是指通过少量的样本让模型可以迁移到不同的下游任务或者领域，通常通过微调的手段。

### **3.Unified Image-Text Modeling**

### Unified Image-Text Modeling，即统一的图像-文本建模，由于大规模多模态预训练的发展。人们想要设计一个统一的模型来支持各种下游任务，包括封闭集分类、开放式文本生成、框/蒙版定位和像素预测。

### **(1)图像-文本任务的分类**:

- •

  **封闭集分类任务：** 如视觉问答（VQA）、图像-文本检索和视觉推理。

- •

  **开放式文本生成任务：** 如图像字幕生成、视觉叙事和自由形式的开放式VQA。

- •

  **框/蒙版定位任务：** 如短语定位、指代表达理解/分割和定位字幕。

- •

  **像素预测任务：** 如文本到图像生成和基于文本的图像编辑。

### **(2)不同方法的总结:**

1. 1.

   **以文本生成为统一任务的方法：**

   - •

     VL-T5：使用序列到序列（seq2seq）编解码器框架，将不同的视觉语言（VL）任务统一为文本生成任务。

   - •

     SimVLM：提出了一个简单的端到端的seq2seq学习框架，也将VQA视为文本生成任务。

2. 2.

   **将文本生成和框预测统一为语言建模的方法：**

   - •

     Pix2Seq 和 Pix2SeqV2：将物体检测（OD）视为语言建模任务。

   - •

     UniTAB：通过将每个边界框表示为一组离散标记，将文本生成和边界框预测统一到单一的Transformer编解码器架构中。

3. 3.

   **将文本生成和图像生成统一为语言建模的方法：**

   - •

     Taming Transformer、DALLE 和 Parti：展示了将图像表示为一系列离散图像标记的方法。

   - •

     ERINE-ViLG、L-Verse、DU-VLG：展示了如何将图像生成和文本生成（如图像字幕）统一起来。

4. 4.

   **将文本生成、框预测和图像生成统一起来的方法：**

   - •

     OFA：提出将文本生成、框预测和图像生成统一起来的方法。

   - •

     Unified-IO：支持图像、蒙版、关键点、框和文本等多种模态，以及深度估计、修图、语义分割、字幕和阅读理解等多种任务。

5. 5.

   **定位和视觉语言理解的统一：**

   - •

     一些工作（如GPV-1、MDETR、GLIPv2 和 FIBER）尝试统一定位和视觉语言理解任务，但仍使用额外的OD头来输出边界框。

## **(二）dual encoder**