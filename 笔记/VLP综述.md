- # **VLP模型综述**

  ## **一.VLP模型的历史**
  
  1.**Small-scale task-specific method design (2014/11-2019/8)**：这段时间的技术主要针对特定任务进行开发。
  
  像针对image captioning和VQA出现的技术主要有：使用ResNet，Faster RCNN这种深度学习框架提取视觉特征；基于预训练词嵌入的方法，它将词语或短语转换为长度固定的向量；针对序列数据的LSTM模型；注意力机制等。
  
  **2.Medium-scale pre-training (2019/8-2021/8)**: 这段时间受到BERT在NLP领域的巨大成功，VL模型逐渐转向基于使用Transformer的多模态融合模型，并且开始在中等规模的数据集上进行预训练，比如使用大约10M的图像-文本对来训练，比如UNITER模型以及OSCAR模型。
  
  **3.Large-scale pre-training (2021/8-now):** 随着CLIP (Radford et al., 2021) 和 ALIGN (Jia et al., 2021) 的出现，大规模的视觉语言预训练（VLP）显示出巨大的潜力，并成为视觉-语言（VL）研究的基础。VLP的高计算成本可以通过将预训练模型适配到广泛的下游任务中来分摊。这时候数据集已经可以达到12B，模型参数达到5B。
  
  ## **二.VLP模型所面临的代表性Task**
  
  VLP模型所面临的任务大致可以分为三类，Image-Text Tasks（图像-文本任务），Computer Vision Tasks（计算机视觉任务，老任务，迁移过去），Video-Text Tasks（视频-文本任务）
  
  ## **(一）Image-Text Tasks**
  
  ### **1.Image-text Retrieval**
  
  **(1)类别**：一般分为两个子任务，一个是给定图像，检索相关文本的image-to-text retrieval；另一个是给定文本，检索相关图像的text-to-image retrieval。
  
  **(2)评估指标**：Recall@K (K=1, 5, 10)
  
  Recall@K的定义是：在前K个检索结果中，有多少比例的相关项被模型检索到。
  
  - •
  
    **Recall@1** 指的是模型在其返回的第一个结果中检索到相关项的概率。
  
  - •
  
    **Recall@5** 指的是模型在其返回的前五个结果中检索到相关项的概率。
  
  - •
  
    **Recall@10** 指的是模型在其返回的前十个结果中检索到相关项的概率。
  
  **(3)数据集**：COCO，Flickr30K
  
  ### **2.Visual Question Answering and Visual Reasoning**
  
  **(1)两个任务的定义联系于区别**
  
  ### **VQA任务**
  
  **定义：**
  
  - •
  
    VQA任务涉及理解给定的图像并对与该图像相关的自然语言问题给出回答。这要求模型能够识别图像中的对象、活动、场景等元素，并理解问题中的语言信息。
  
  **特点：**
  
  - •
  
    VQA任务通常集中于直接的图像内容理解，如识别、分类或描述图像中的物体。
  
  - •
  
    问题可能涉及图像的具体细节，如“照片中的狗是什么颜色的？”或者对图像中的活动进行简单的描述。
  
  ### **Visual Reasoning任务**
  
  **定义：**
  
  - •
  
    Visual Reasoning任务要求更深层次的理解和推理，不仅仅是识别图像中的物体，而是理解物体之间的关系、图像中的动作或事件以及潜在的情境和上下文。
  
  **特点：**
  
  - •
  
    这类任务通常涉及更复杂的推理过程，比如理解空间关系、执行多步骤的逻辑推理或对图像内容的深层含义进行推断。
  
  - •
  
    例如，Visual Reasoning可能包括理解物体间的相互作用或对图像情境进行常识推理。
  
  ### 联系与区别
  
  **联系：**
  
  - •
  
    Visual Reasoning任务可以看作是VQA任务的一个子集或扩展，因为它们都涉及到解释图像内容和理解与图像相关的问题。
  
  - •
  
    两者都要求模型具备视觉理解和语言处理的能力。
  
  **区别：**
  
  - •
  
    VQA任务的重点通常在于直接的图像内容识别和基础问题回答，而Visual Reasoning任务则要求更高级的认知处理，如推理、判断和解释。
  
  - •
  
    Visual Reasoning任务通常更具挑战性，需要对图像中的细微差别、复杂情境和隐含的逻辑关系有深入的理解。
  
  **(2)评估指标：**
  
  ![4e3c13bd2781fdff4becea5b97274c8.png](https://s.readpaper.com/T/2X7mhRWYHQX)
  
  也就是说当至少有三个人类给出和机器一样的答案时，得分就会较高，说明有较高的“人类一致性”。
  
  ### **3.Image Captioning**
  
  **(1)定义:** Image Captioning 是为给定图像生成自由形式的文本标题。
  
  **(2)评估指标:**
  
  **基于n-gram（词的组合）重叠来评估生成的文本与参考文本（通常是人类编写的描述）之间的相似度**:
  
  什么是n-gram:比如说生成的句子是“我喜欢猫”，而参考文本是“我喜欢狗”，现在我们使用2-gram，首先会将生成文本和参考文本以两个单词为单位进行分割，生成句子会被分割成“我喜”，“喜欢”，“欢猫”，而参考文本会生成“我喜”，“喜欢”，“欢狗”，之后会根据不同的n-gram评价指标计算指标，例如如果使用BLEU评价指标，就会是相同的n-gram除以总的n-gram数量，在这里也就是2/3。
  
  n-gram评价指标有：BLEU，METEOR，ROUGE-L，CLDEr
  
  **基于语义的评价指标。这种度量方法专注于评估生成的文本在语义内容上与参考文本的一致性。**
  
  从上边的例子也可以看出，“我喜欢猫”和“我喜欢狗”明明有很大区别，但最后相似度却很高，这就是n-gram的缺点，没有考虑语义，所以有了这种基于语义的评估指标。
  
  语义评价指标：SPICE，它通过分析文本中的命题（propositions）— 如主体、谓语和宾语 — 来衡量语义相似度。
  
  **(3)数据集：**
  
  single-sentence captions: COCO, TextCaps, No-Caps
  
  multi-sentence captions: 少有人制作复杂数据集。
  
  ## **(二）Computer Vision Tasks**
  
  计算机视觉任务就是一些比较经典的任务，比如图像分类，语义分割等。
  
  **(1)Image Classification(图像分类）**
  
  - •
  
    **目标：** 识别图像中的主要对象或场景。
  
  - •
  
    **优势：** VLP模型通过图像描述或相关文本标签来增强分类。
  
  **(2)Object Detection(目标检测)：**
  
  - •
  
    **目标：** 在图像中识别和定位多个对象。
  
  - •
  
    **优势：** VLP模型可以通过理解与对象相关的文本信息来改善检测。
  
  **(3)Semantic Segmentation（语义分割）：**
  
  - •
  
    **目标：** 对图像中的每个像素进行分类，用于分割图像中的不同区域或对象。
  
  - •
  
    **优势：** VLP模型可以通过文本提示来识别和分割特定类别的对象。
  
  ## **三.Task-specific VL Models**
  
  这段时间的模型通常由以下部分组成：图像编码器，文本编码器，多模态融合模块，针对特定任务的输出层。
  
  ### **1.Visual Encoder**: 这段时间主要就是两个种类模型作为图像编码器。
  
  **(1)CNN:** 由于当时使用CNN进行图像分类十分成功，所以当时的VL任务使用在ImageNet上进行预训练过的CNN网络（如VGGNet，ResNet等）作为Visual Encoder，提取最后一层特征作为全局图像特征。之后为了保留空间信息，开始使用模型前几层所产生的特征，称为grid features。
  
  **(2)OD:** OD指的是Object Detection（目标检测），它可以检测图片中所有物品并利用方框进行定位，比较典型的例子是Faster RCNN，当我们使用OD作为Visual Encoder的时候指的是使用OD进行物体检测，并针对每个物体生成框，之后针对每个框提取物体图像特征（也称为区域特征 region feature），最后根据所问问题来找到具体的物体框的特征来回答问题。
  
  **2.Text Encoder:** Text Encoder会将我们输入的句子先分词，再进行词嵌入，将每个词转换为词向量，之后应用模型进行文本编码，早期的文本编码模型有三类。
  
  **(1)BoW(Bag-of-Words):** 在Bow中，每个单词被独立编码，通常不考虑它们之间的顺序或语境关系。这些编码后的单词向量可能会被求和或取平均来生成问题的最终表示。
  
  **(2)RNN:** 也就是我们所说的循环神经网络，除了单词的编码之外，还考虑了单词之间的顺序，充分考虑了上下文信息。
  
  **(3)Transformer:** 由于BERT的出现，Transformer模块被广泛用于提取文本特征，这种方法使用注意力机制，不仅能够捕捉足够长的上下文关系，同时提供了高效率的并行计算能力。
  
  ###### **3.Multimodal Fusion Module：** 多模态融合模块，旨在建立视觉特征和文本特征之间的交互作用，以更好地理解和处理包含图像和文本的数据。
  
  ### **(1)简单融合**
  
  - •
  
    **方法：** 图像和文本特征通过元素级乘积、求和或连接进行融合。
  
  - •
  
    **进一步细化：** 一些更复杂的设计通过使用LSTM或多模态残差网络来细化融合后的图像-文本特征。
  
  ### **(2)跨模态注意力(Inter-modality attention)**
  
  - •
  
    **目的：** 捕捉图像和文本输入之间的多模态对齐。
  
  - •
  
    **方法：**
  
    - •
  
      **叠加注意力网络（SAN）：** 在VQA中首次验证了跨模态注意力的有效性，使用问题作为查询来关注图像特征。
  
    - •
  
      **共注意力方法：** 同时进行问题引导的图像注意力和图像引导的文本注意力。
  
    - •
  
      **双线性注意力（BAN）：** 考虑问题中每个单词和图像区域之间的每一对组合。
  
    - •
  
      **多层注意力：** 可以视为多步推理过程，注意力分布逐层细化，专注于与问题更相关的区域。
  
  ### **(3)内部模态注意力(Intra-modality attention)**
  
  - •
  
    **目的：** 在图像区域或问题单词之间进行关系推理,主要针对单模态，帮助模型识别图像或文本最重要的地方。虽然他是但模态，但这种方法结合跨模态模块可以达到更好的效果。
  
  - •
  
    **方法：** 通过构建图结构表示（例如，使用依赖解析构建的问题中的单词图，或利用外部知识和基于规则的先验构建的图像中的对象区域图）来提高VQA性能。
  
  ### **(4)Transformer**
  
  - •
  
    **特点：** 不仅通过跨模态注意力关注其他模态，还通过内部模态注意力关注当前模态中的相关区域或单词。(跨模态+内模态）
  
  - •
  
    **应用：** MCAN模型使用Transformer架构，其中编码器使用自注意力单元进行内部模态交互（区域到区域或单词到单词），解码器则使用引导注意力单元进行密集的跨模态交互。
  
  ### **4.Task-specific Output Layer**：针对不同的任务要使用不同的预测层
  
  比如 VQA 通常被建模为分类问题，输出层是一个由全连接层或多层感知器和 softmax 层组成的分类器来预测答案。
  
  ## **四.VLP Models**
  
  受到语言模型预训练的成功以及语言模型和视觉模型开始统一都使用Transformer，VLP模型开始流行。(在第一部分说的**Medium-scale pre-training**和**Large-scale pre-training**都属于VLP领域）
  
  ## **1.模型分类**
  
  VLP模型大概可以分为两类，按照如何融合视觉与文本特征分为dual encoder和fusion encoder。
  
  ## **(一)fusion encoder**
  
  fusion encoder是指除了使用image encoder和text encoder来提取文本和图像特征之外，还会额外使用一个Transformer layers来对文本特征和图像特征进行深层的融合。
  
  fusion encoder可以分为两类，一类是两阶段训练的two-stage pre-training pipeline，另一类是端到端的end-toend pre-training。
  
  ## ***1.模型分类***
  
  ### **1.two-stage pre-training**
  
  **(1)定义**： 这其中最为经典的模型就是OD-based VLP Models，会首先使用预训练好的目标检测模型（如Faster-RCNN）来提取区域特征，之后通过Transformer模块来提取融合文本和区域特征。
  
  ### **2.End-to-End VLP Models**
  
  **(1)定义**：端到端的模型可以通过卷积神经网络（CNNs）、视觉Transformer（ViTs）或直接使用图像块嵌入来提取图像特征，而不需要像两阶段模型一样提前提取图像区域特征。
  
  **(2)方法：**
  
  **CNN-based Grid** 
  
  - •
  
    **模型示例：** PixelBERT和CLIP-ViL。
  
  - •
  
    **方法：** 这些模型直接将CNN提取的网格特征和文本送入一个Transformer模型。
  
  - •
  
    **特点：** 使用网格特征可以提高效率，但通常需要为CNN和Transformer使用不同的优化器。例如，PixelBERT和CLIP-ViL分别为Transformer使用AdamW优化器，为CNN使用SGD优化器。
  
  **ViT-based Patch Features**
  
  - •
  
    **模型示例：** ViLT、ViTCAP、UFO、VLMo。
  
  - •
  
    **方法：** 这些模型使用Transformer模型来提取图像特征。
  
  ## ***2.模型组成***
  
  在fusion encoder中，模型架构可以分为三部分，视觉编码器，文本编码器，多模态融合器
  
  ### **1.Vision Encoder**
  
  **(1)OD:** OD主要应用于two-stage pre-training(放入Transformer模块前提取区域特征），其中最为流行的是faster-rcnn。 之后为了加入位置信息，提高性能，VinVL提出了一种基于ResNeXt-152 C4架构的OD模型，并得到了显著的性能提升。
  
  **(2)CNN**:VLP中使用的CNN提取更通用的图像特征，基本使用的都是Resnet模型及其变体。并且我们发现更强大的CNN模型能让网络在下游任务中有更好的表现。
  
  - •
  
    PixelBERT、SOHO和CLIP-ViL等模型中，使用了如在ImageNet上预训练的ResNet-50、ResNet-101和ResNeXt-152来提取图像特征。
  
  - •
  
    CLIP-ViL中，使用了从CLIP模型预训练的ResNet变体。
  
  - •
  
    SimVLM使用了ResNet-101和ResNet-152的前三个块（排除了Conv stem）来构建其基础和大型模型，并使用了通道数更多的ResNet-152变体来构建巨型模型。
  
  **(3)ViT:**  即Vision Transformer,首先将图像分割成图像块，然后将这些块展平成向量并线性投影以获得块嵌入。并将一个可学习的特殊token（[CLS]）嵌入也被添加到序列的开头，之后送入多层Transformer模块以获得最终的输出图像特征。
  
  - •
  
    **ViT变体：** 原始的ViT、DeiT、BEiT、Swin Transformer和CLIP-ViT等。
  
  ### **2.Text Encoder**
  
  **(1)使用模型：** 使用的模型大同小异，都是BERT或它的变体，如RoBERTa、ELECTRA、ALBERT、DeBERTa。
  
  **(2)处理过程：** 先将句子进行分割，之后在句首和句尾加上特殊标记，生成文本序列，之后再放入模型中进行处理生成文本特征。
  
  **(3)文本表示：** 无论使用哪种文本编码器，输入文本最终都被表示为一组特征向量集合。
  
  ### **3.Multimodal Fusion**
  
  **(1)merged attention:** 在这种模块中，文本和视觉特征被简单地连接在一起，然后送入单个Transformer块。这种设计被用于VisualBERT、Unicoder-VL、VLP、VLBERT、UNITER、OSCAR、VinVL、ViLT、GIT等模型中。
  
  **(2)co-attention:** 文本和视觉特征分别独立地送入不同的Transformer块，使用如交叉注意力等技术来实现跨模态交互。这种设计被用于LXMERT、ViLBERT、ERNIE-ViL、METER等模型中。
  
  ![image.png](https://s.readpaper.com/T/2XGAzkUDH7J)
  
  **(3)对比：** merged attention的参数效率更高，对于end-to-end型的模型，使用Co-attention表现得更好，所以两个方法都各有优劣，需要根据具体模型选择。
  
  ## ***3.模型架构***
  
  ### **1. 仅编码器架构**
  
  - •
  
    **应用：** 大多数VLP模型采用仅编码器架构。在这种架构中，跨模态表示直接输入到基于多层感知机（MLP）的输出层，以生成最终输出。
  
  - •
  
    **适用任务：** 这种设计自然适合于视觉语言理解任务，如视觉问答（VQA）和视觉推理。
  
  - •
  
    **用于图像描述：** 当用于图像描述生成时，同一个编码器充当解码器的角色，通过使用因果掩码（causal mask）逐个令牌地生成输出字幕。
  
  ### **2. 编码器-解码器架构**
  
  - •
  
    **应用：** 在这些模型中，跨模态表示首先输入到解码器，然后再到输出层。解码器关注编码器的表示和之前生成的令牌，以自回归方式产生输出。
  
  - •
  
    **优势：** 使用编码器-解码器架构可以实现对各种图像-文本任务的统一，以及VLP模型的零样本/小样本学习，并且自然适合于生成任务。
  
  ## ***4.Pre-training Task***
  
  **定义：** 预训练任务是用于训练大型视觉语言模型之前的一系列任务，目的是在开始针对具体下游任务进行微调之前，先在大量未标注或部分标注的数据上学习通用的特征和表示。有着学习通用特征，改善下游任务表现的作用。
  
  **(1)Masked Language Modeling(MLM):** 
  
  - •
  
    **定义**：给定一个图像-文本对，随机遮盖输入文本中约15%的词，并用特殊的[MASK]标记替换这些被遮盖的词。目标是基于它们周围的词和配对的图像来预测这些掩码词。
  
  - •
  
    **应用**：
            **Seq-MLM：** 被[Mask]的地方只能通过它之前的内容作为依据来预测。
  
    ​        **LM：** 模型从一个初始状态（可能是完全空白或包含几个启动词）开始，然后逐步生成后续的词汇，模型在生成每个新词时都依赖于之前已生成的所有词汇。
  
    ​        **Prefix-LM：** 把被[Mask]的句子分成两部分，带[Mask]的一般在后半部分，使用双向注意力处理“前缀”和图像，了解上下文信息。使用单向注意力预测[Mask]的词。
  
  **(2)Image-Text Matching(ITM):**
  
  - •
  
    **定义：** 在ITM任务中，模型需要判断给定的图像和文本（字幕）对是否相互匹配。
  
  - •
  
    **流程：** 在输入句子的开始处添加一个特殊标记（如[CLS]），用于学习全局的跨模态表示。之后输入匹配或不匹配的图像-文本对〈v, w〉，其中v和w分别代表图像和文本的特征表示。之后在[CLS]标记上添加一个分类器，用来预测二元标签ys，表明所提供的图像-文本对是否匹配。
  
  **(3)Image-Text Contrastive Learning(ITC):**
  
  - •
  
    **定义：** 给定一个包含N个图像-文本对的批次，ITC的目标是从所有N^2个可能的图像-文本组合中预测出N个匹配对。
  
  **(4)Masked Image Modeling(MIM)**
  
  - •
  
    **定义：** 是在图像领域中对MLM的类比，模型需要根据剩余的可见图像区域或块和所有文本来重构被掩码的图像区域。
  
  - •
  
    **分类：**
  
    **基于物体检测器（OD）的VLP模型中的MIM：**
  
    - •
  
      **操作方式：** 在这类模型（如LXMERT和UNITER）中，一些输入区域(region)被随机掩码（用零替换其视觉特征），模型通过最小化均方误差损失来重构原始区域特征。
  
     **端到端VLP模型中的MIM：**
  
    - •
  
      **掩码块回归/分类：** 在ViLT和METER等模型中，研究者探索了对掩码图像块进行回归或分类的MIM方法。
  
    - •
  
      **离散VQ标记的MIM：** 受BEiT启发，使用VQ-VAE模型将每个图像标记为一系列离散标记，模型训练以重构这些离散标记。
  
    - •
  
      **批内负样本的MIM：** 类似于MLM使用文本词汇表，模型通过动态构建的批内负样本词汇表来重构输入块
  
  ## ***5.Pre-training Datasets***
  
  ### **(1)学术环境下的预训练数据**集:
  
  - •
  
    **常用数据集：** 在典型的学术环境中，VLP模型通常在四个常用的图像-字幕数据集上进行预训练，包括COCO、Visual Genome (VG)、Conceptual Captions (CC3M) 和 SBU Captions。
  
  - •
  
    **数据集特点：**
  
    - •
  
      COCO和VG被视为“域内”数据集(In-domain datasets)，因为大多数视觉语言下游任务都是基于它们构建的。
  
    - •
  
      CC3M和SBU Captions被视为“域外”数据集(Out-of-domain datasets)。
  
    - •
  
      域内域外数据集主要看数据集中数据是否与模型将要执行的任务或应用场景非常相似
  
  - •
  
    **其他数据集：** 最近，还有使用更大的数据集，如CC12M和Localized Narratives，后者的字幕更长，包含更丰富的描述。
  
  ### **(2)工业设置下的预训练数据集:**
  
  - •
  
    **大规模网络抓取数据集：**
  
    - •
  
      CLIP使用的数据集包含4亿个图像-文本对，基于50万个查询构建。
  
    - •
  
      ALIGN使用的数据集有18亿个图像-文本对，数据收集管道类似于CC，但清理步骤更宽松。
  
    - •
  
      Wikipedia-based Image-Text Dataset (WIT) 包含1150万个独特图像和3760万个文本，特点是多语言。
  
    - •
  
      WenLan包含3000万个图像-文本对，经过精心清理。
  
    - •
  
      LAION-400M/5B包含4亿或50亿个图像-文本对，使用CLIP模型进行过滤。
  
    - •
  
      RedCaps包含1200万个图像-文本对，来自350个subreddits。
  
    - •
  
      Florence和GIT使用的数据集包含8亿个图像-文本对，包括ALT200M。
  
  ## ***6.Advanced Topics***
  
  随着VLP的快速发展与完善，近些年出现了一些新的研究方向，这些方向都是**最新**的。
  
  ### **1.Big Models**
  
  随着NLP领域的模型参数的迅速增长，VLP领域也展现出了相同的趋势，参数迅速增长。不同的Big Models采用不同的模型组件与架构，不同大小的数据集，不同的预训练任务，这些模型所擅长的下游任务与性能也有所不同。
  
  ![image.png](https://s.readpaper.com/T/2XNWah1WsQj)
  
  **2.In-Context Few-Shot Learning**
  
  In-Context Few-Shot Learning就是指通过少量的样本让模型可以迁移到不同的下游任务或者领域，通常通过微调的手段。
  
  ### **3.Unified Image-Text Modeling**
  
  ### Unified Image-Text Modeling，即统一的图像-文本建模，由于大规模多模态预训练的发展。人们想要设计一个统一的模型来支持各种下游任务，包括封闭集分类、开放式文本生成、框/蒙版定位和像素预测。
  
  ### **(1)图像-文本任务的分类**:
  
  - •
  
    **封闭集分类任务：** 如视觉问答（VQA）、图像-文本检索和视觉推理。
  
  - •
  
    **开放式文本生成任务：** 如图像字幕生成、视觉叙事和自由形式的开放式VQA。
  
  - •
  
    **框/蒙版定位任务：** 如短语定位、指代表达理解/分割和定位字幕。
  
  - •
  
    **像素预测任务：** 如文本到图像生成和基于文本的图像编辑。
  
  ### **(2)不同方法的总结:**
  
  1. 1.
  
     **以文本生成为统一任务的方法：**
  
     - •
  
       VL-T5：使用序列到序列（seq2seq）编解码器框架，将不同的视觉语言（VL）任务统一为文本生成任务。
  
     - •
  
       SimVLM：提出了一个简单的端到端的seq2seq学习框架，也将VQA视为文本生成任务。
  
  2. 2.
  
     **将文本生成和框预测统一为语言建模的方法：**
  
     - •
  
       Pix2Seq 和 Pix2SeqV2：将物体检测（OD）视为语言建模任务。
  
     - •
  
       UniTAB：通过将每个边界框表示为一组离散标记，将文本生成和边界框预测统一到单一的Transformer编解码器架构中。
  
  3. 3.
  
     **将文本生成和图像生成统一为语言建模的方法：**
  
     - •
  
       Taming Transformer、DALLE 和 Parti：展示了将图像表示为一系列离散图像标记的方法。
  
     - •
  
       ERINE-ViLG、L-Verse、DU-VLG：展示了如何将图像生成和文本生成（如图像字幕）统一起来。
  
  4. 4.
  
     **将文本生成、框预测和图像生成统一起来的方法：**
  
     - •
  
       OFA：提出将文本生成、框预测和图像生成统一起来的方法。
  
     - •
  
       Unified-IO：支持图像、蒙版、关键点、框和文本等多种模态，以及深度估计、修图、语义分割、字幕和阅读理解等多种任务。
  
  5. 5.
  
     **定位和视觉语言理解的统一：**
  
     - •
  
       一些工作（如GPV-1、MDETR、GLIPv2 和 FIBER）尝试统一定位和视觉语言理解任务，但仍使用额外的OD头来输出边界框。
  
  ## **(二）dual encoder**
  
  针对dual encoder，作者主要结合传统Vision Tasks进行讨论，我在查阅一些资料后觉得这是因为fusion encoder会深层次的融合特征，它对文本和图像有更深层次的理解，所以它能更好的解决Image-text task这种VLP专门的任务，因为这里边涉及到如VQA，IM这种需要推理理解图片意思的任务在。
  
  而dual encoder在面对视觉和文本特征时，只会对它们进行简单的处理，比如点积或者用余弦相似度，它们没有更深层次的探索文本与图像间的特征，但是这也让他们的速度变得很快。而针对单纯的Vision Tasks时，本身我们使用了text这个模态的时候就已经对这些任务带来了很大的增益，不需要更深层次的探索文本图像间的联系，并且这些任务本身就是对速度更加有要求，所以我们更多选择dual encoder类模型来进行Vision tasks。
  
  ## ***1.Image Classification***
  
  **1.Language-Image Pre-training方法在Image Classification有哪些创新和进展:**
  
  ### **(1)改进的对比学习目标**:
  
  1. 1.
  
     **FILIP**：通过精细化区域-词对应关系来提升模型性能。
  
  2. 2.
  
     **PyramidCLIP**：构建具有不同语义层次的输入金字塔，并通过内部和跨层对齐来对齐两种模态。
  
  3. 3.
  
     **前缀条件化**：根据数据类型，引入前缀提示以结合图像-字幕和图像-标签数据。
  
  4. 4.
  
     **CyCLIP**：通过明确对称化两个不匹配的图像-文本对之间的相似度（跨模态一致性），以及图像-图像对和文本-文本对之间的相似度（内模态一致性），来学习一致的表示。
  
  ### **(2)自监督+对比目标:**
  
  1. 1.
  
     **DeCLIP**：在图像-文本对中探索单模态自监督信号。
  
  2. 2.
  
     **SLIP**：结合图像到图像的自监督学习和图像到文本的对比学习。
  
  3. 3.
  
     **结合掩码图像/语言建模和图像到文本对比学习**，如MultiMAE和M3AE。
  
  ### **(3)冻结模型:**
  
  1. 1.
  
     **LiT**：引入“对比调整”方法，表明锁定预训练的图像编码器并调整文本编码器在零样本迁移中表现最佳。
  
  2. 2.
  
     **Flamingo**：利用单模态预训练模型，并继续预训练跨模态模块，实现了使用上下文学习的卓越图像分类性能。
  
  ### **(4)规模扩大:**
  
  1. 1.
  
     **BASIC**：提出扩大CLIP和ALIGN的对比学习框架的三个维度：数据规模、模型规模和批量大小，实现85.7%的零样本准确率在ImageNet上。
  
  2. 2.
  
     **LIMoE**：是一种能进行语言-图像多模态学习的稀疏混合专家模型。
  
  3. 3.
  
     **PaLI**：发现联合扩大视觉和语言组件很重要，训练了迄今为止最大的ViT来量化更大容量视觉模型的好处。
  
  ### **2.两种实验设置评估预训练模型的开放集图像分类能力**:
  
  1. 1.
  
     **类级别迁移（单域内）**：在给定视觉领域内预定义手动拆分，确保训练中未观察到评估概念。
  
  2. 2.
  
     **任务级别迁移**：CLIP直接应用预训练检查点来识别约30个公共图像分类数据集中的任何概念。
  
  ### **3.图像-语言模型如何在IC中使用:**
  
  ### 如何使用像CLIP这样的图像-文本对比训练模型进行零样本图像分类。
  
  - •
  
     给定一个新的IC数据集/任务和一组概念/类别名称，每个概念被转换为字幕，然后用作文本编码器的提示以提取概念表示。查询图像被输入到图像编码器以提取视觉表示，然后用于计算与所有概念的相似度。相似度最高的概念成为预测的概念。
  
  ## ***2.Object Detection***
  
  传统的Object Detection 任务分为两部分，定位和分类，定位任务就按照传统计算机视觉模型就可以完成，重点在于分类任务，它类似于IM任务，也就是说针对可能的候选框，我们可以给它文本描述，这样就可以像IM那样进行分类。
  
  由于像CLIP这样的模型已经学习到了图像-文本之间的配对关系，这样就可以让分类任务像开放集转变，因为传统的Object Detection会利用object/box/region这样的东西将所有可能潜在的目标标识出来，而CLIP又学习到了文本图像间的特征关系，所以即使从来没见过的东西，CLIP一样可以进行分类。
  
  **(1)One-stage Models**
  
  在定位任务上来说，可能是通过anchor或者其他方法标出潜在的object/box/region，但进行分类任务时，模型会有一个text prompt ‘t’，t由希望预测的全部的类别的K个短语所组成。之后由visual encoder和text encoder对图像和‘t'进行编码，提取特征，之后进行点积计算相似度。
  
  ![image.png](https://s.readpaper.com/T/2XfdrP4RIJN)
  
  - •
  
    **应用:** GLIP
  
  **(2)Two-stage Models**
  
  两阶段模型与单阶段模型最大的不同是在于定位部分，两阶段模型会使用如RPN这样的区域提议模型，找出可能的box/object/region。在分类部分和单阶段方法类似。
  
  - •
  
    **应用:** RegionCLIP
  
  **(3)其他的语言-图像预训练方法**:
  
  - •
  
    一些类似于GLIP的模型，它们将目标检测视为短语定位问题，并使用文本查询来处理整个图像。例如，MDETR 使用单一文本查询；FIBER 改进了GLIP，采用粗到细的预训练管道，并在主干网络中而不是在目标检测头部进行融合；OVR-CNN 在有限的词汇上微调图像-文本模型，并依赖图像-文本预训练进行开放词汇的泛化；Detic 通过仅在只有图像级注释的示例上训练分类头部来提高长尾检测性能。还提到了其他一些工作，如OV-DETR、X-DETR、FindIT、PromptDet 和 OWL-ViT。
  
  **(4)开放集目标检测的评估设置**:
  
  - •
  
    类别级别转移（Class-level Transfer）：在单一领域中预定义手动分割，确保训练和评估之间没有概念重叠。
  
  - •
  
    任务级别转移（Task-level Transfer）：在零样本设置下评估预训练目标检测模型在多个数据集上的性能
  
  **3.Semantic Segmentation**
  
  **(1)语言驱动的语义分割（LSeg）**:
  
  - •
  
    使用LSeg（Li et al., 2022a）作为例子来说明图像分割过程。在这个过程中，文本类别和图像像素被嵌入到一个共同的空间中，每个像素被分配到一个语义类别。
  
  - •
  
    对于具有 *K* 个类别标签的语义分割任务，文本编码器将这些标签嵌入到一个连续的向量空间 R*d* 中，产生所有类别的嵌入矩阵 *P*=[*p*1,…,*pK*]∈R*K*×*d*。对于图像 *x*，图像编码器将其编码成一个密集的网格表示 *O*∈R*H*×*W*×*d*，其中 *H* 和 *W* 指定特征图的空间大小。
  
  - •
  
    计算单词-网格相似性张量 S=*OP*⊤∈R(*H*×*W*)×*K*。即为每个像素对应每个类别的相似度，之后每个像素挑选相似度最高的类别作为该像素的类别。
  
  **(2)基于CLIP的分割模型**:
  
  - •
  
    提到许多分割模型直接适应预训练的CLIP模型到像素级视觉识别任务，包括PhraseCut、OpenSeg、CLIPSeg、ZS-Seg、MaskCLIP、DenseCLIP 等。例如，OpenSeg还执行了使用类别不可知的掩码注释进行模型学习，以生成掩码提议。
  
  **(3)从头开始训练**:
  
  - •
  
    GroupViT是一种新的分层分组Transformer架构，利用Transoformer的全局自注意机制将输入图像划分为逐渐变大的任意形状的组。它在约1200万的图像-文本对上进行预训练，并使用多标签图像-文本对比损失。由于GroupViT自动将图像分组为语义上相似的段，因此它的输出可以轻松转换为语义分割，而无需微调
  
  ### **4.Trends: From Close-set to Open-set, to in-the-Wild**
  
  视觉域：指的是图像数据的视觉特性和来源的多样性。不同的视觉域可能包括不同的环境、背景、光照条件、图像质量、拍摄角度等。例如街道视图，医疗图像，卫星图像都属于不同的视觉域。
  
  概念域：指的是图像中可以识别和分类的对象、场景或概念的多样性。例如植物和动物就属于不同的概念域。
  
  任务级别的转移：Task-Level Transfer，指的是模型能够从一个任务转移到另一个不同的任务的能力，例如能从IM任务转移到OB任务。
  
  **(1)传统封闭集识别设置（Traditional Close-Set Recognition Setting）**:
  
  - •
  
    在这个设置中，训练和测试数据都来自相同的类别集合。模型在训练时见过所有类别，测试时需要识别这些已知类别的新实例。
  
  - •
  
    这是最常见的机器学习和计算机视觉任务设置。
  
  - •
  
    视觉域，概念域，任务域均为跨越
  
  **(2)开放集识别设置（Open-Set Recognition Setting）**:
  
  - •
  
    在开放集识别中，测试时会出现训练阶段未见过的新类别。模型需要能够处理这些未知类别，通常通过识别它们为“未知”或将它们分类为最接近的已知类别。
  
  - •
  
    这种设置对于实际应用更具挑战性，因为它需要模型具有更好的泛化能力。
  
  - •
  
    视觉域未跨越，概念域有一定跨越，任务域未跨越
  
  **(3)域适应或分布外设置（Domain Adaptation or Out-of-Distribution Setting）**:
  
  - •
  
    这个设置关注于当训练数据和测试数据来自不同的分布或视觉域时的情况。例如，模型可能在一个数据集上训练，然后在完全不同的数据集上进行测试。
  
  - •
  
    这要求模型能够适应或转移学习到新的、未见过的数据分布。
  
  - •
  
    视觉域跨越，概念域有一定跨越，任务域未知
  
  **(4)计算机视觉在CVin设置（CVinW, Computer Vision in the Wild Setting）**:
  
  - •
  
    CVinW考虑了视觉域和概念域的变化。它不仅仅关注类别级别的转移（如在开放集识别中），而是关注任务级别的转移。
  
  - •
  
    这意味着在CVinW设置中，模型需要能够在多个不同的数据集、场景和类别上进行有效的识别，这些数据集可能在视觉特征和类别组成上都有显著差异。
  
  - •
  
    两个目标：**广泛的任务迁移能力**，**低成本的任务迁移适应性**
  
  - •
  
    视觉域，概念域，任务域均有跨越
  
  ### **5.Advanced Topics**
  
  **(1)知识增强的视觉模型（Knowledge-Augmented Visual Models）**:
  
  - •
  
    文本编码器是最近发展的语言增强计算机视觉系统中独特的组成部分。K-LITE 丰富了自然语言中的实体与WordNet/Wikipedia知识库，为图像分类和目标检测提供了一种可扩展的方法，以零样本和少样本的方式迁移到大量新任务。与CLIP/UniCL/GLIP相比，K-LITE在预训练中更加高效。
  
  **(2)多语言语言-图像对比（Multilingual Language-Image Contrast）**:
  
  - •
  
    使用英文字幕进行图像-文本对比学习的成功激发了使用其他语言资源的尝试。MURAL 在多语言图像-文本对上从头开始进行预训练，采用图像-文本和文本-文本的对比损失。Carlsson 等人通过从原始的英文CLIP蒸馏，训练特定语言的编码器，同时保持其图像编码器固定。
  
  **(3)高效适应方法（Efficient Adaptation Methods）**:
  
  - •
  
    随着模型大小的增长，如何高效地将预训练模型适应到大范围的下游任务成为一个问题。有关样本效率（如零样本和少样本）和参数效率（如提示调整、线性探测和完整模型微调）的研究正在进行。VLP模型提供了利用文本编码器进行模型适应的独特机会，包括条件提示学习、颜色提示调整（CPT）、VL-Adapter和CLIP Adapter。
  
  **(4)鲁棒性（Robustness）**:
  
  - •
  
    研究了零样本模型的鲁棒微调。Fang等人报告说，数据决定了CLIP中的分布鲁棒性。在分布外设置中，微调CLIP可能会扭曲预训练的特征并表现不佳。原始的CLIP论文报告称，当样本数量较少时，少样本比零样本CLIP效果更差。相反，Li等人展示了当正确使用预训练的文本编码器进行模型适应时，少样本CLIP总是比零样本CLIP效果更好。
  
  **(5)基准（Benchmark）**:
  
  - •
  
    有效转移和公平评估预训练的语言增强视觉模型到下游数据集和任务仍然是一个挑战。ELEVATER 提供了一个用于评估语言增强视觉模型的评估平台，包含一系列数据集和易于使用的工具包，用于评估预训练视觉模型的任务级别迁移能力，这与传统评估类别级别零样本转移的基准不同。
  
  ## **五.VL Systems in Industry**
  
  ### **1.VL在商业领域的应用**
  
  **(1)微软Office中的自动图像描述**：
  
  - •
  
    在Microsoft PowerPoint、Word和Outlook中，插入图片后，可以通过右键点击图片并选择"查看Alt文本"来调用Microsoft Cognitive Service的图像描述API，生成图片的描述。
  
  **(2)Microsoft Edge浏览器的自动生成图片描述**：
  
  - •
  
    Edge浏览器将为没有alt文本的图片提供自动生成的描述。这些描述可以通过屏幕阅读器读出。
  
  **(3)Google Chrome浏览器的无障碍扩展**：
  
  - •
  
    Google Chrome OS提供了一个无障碍扩展，能够为没有alt文本的图片自动生成描述，并通过屏幕阅读器读出。
  
  **(4)Microsoft Seeing AI应用**：
  
  - •
  
    这是一款为视障或低视力人士设计的移动应用程序，可以调用图像描述API生成图像的alt文本，并通过文字转语音引擎朗读出来。
  
  **(5)Facebook的自动生成图片描述**：
  
  - •
  
    Facebook为用户上传的图片提供自动生成的alt文本功能。上传图片时，系统会自动生成图片描述。
  
  **(6)Apple iOS的VoiceOver屏幕阅读器**：
  
  - •
  
    VoiceOver是Apple移动设备操作系统IOS内置的屏幕阅读器，帮助视障用户通过听觉了解屏幕显示的内容。
  
  ### **2.基于云服务的VL模型应用：**
  
  **(1)Microsoft Azure计算机视觉**：提供图像和视频分析，包括对象、人脸识别、成人内容识别和自动生成的文本描述等。
  
  **(2)Google Cloud Vision AI**：提供自定义机器学习模型的自动训练，以及通过REST和RPC APIs访问的强大预训练机器学习模型。
  
  **(3)Amazon Rekognition**：提供预训练和可定制的计算机视觉功能，以从图像和视频中提取信息和洞察。
  
  **(4)阿里巴巴云图像搜索**：使用深度学习和机器视觉捕捉图像特征，然后根据这些信息进行图像搜索。
  
  ### **3.VL模型存在的潜在问题**：
  
  **(1)领域适应（Domain Adaptation）**：
  
  - •
  
    在现实世界场景中，图像通常是不可预测的，并具有很大的变化。因此，模型必须具有对新领域的鲁棒性和良好的泛化能力。虽然在图像分类和目标检测等其他计算机视觉领域有大量关于领域适应的研究，但针对VL模型的领域适应研究相对较少。特别是对于非自然图像（如图表、表格和图解）的处理，仍然是一个未解决的问题。
  
  **(2)服务成本（Serving Cost）**：
  
  - •
  
    除了准确性之外，推理成本和延迟也是实际应用中需要考虑的限制因素。如何在不牺牲准确性的情况下减小模型大小是一个重要问题。例如，研究者们开发了小型VL模型（如MiniVLM），在显著减小模型大小和推理时间的同时保持了高准确性。
  
  **(3**)**公平性和负责任的AI（Fairness and Responsible AI）**：
  
  - •
  
    公平性和负责任的AI是部署VL模型时的另一个关键问题。由于VL数据集通常包含偏见，训练出的模型也可能带有偏见，可能产生具有冒犯性的结果。例如，有研究指出VL数据集中的性别偏见。因此，减少VL模型中的偏见也是一个重要的课题。
  
  **(4)对话AI代理（Conversational AI Agents）**：
  
  - •
  
    公平性和负责任的AI也在对话AI代理的背景下进行研究。VL系统和对话AI代理共享许多问题，例如防御伤害、减轻潜在的有害内容和偏见。VL模型也被集成到AI聊天机器人中，用于社交聊天和任务完成。设计这些AI机器人时需要考虑多种因素，以适应真实世界的挑战。
  
  ## **六.VLP Future**
  
  通用多模态基础模型（general-purpose multimodal foundation models）已经成为未来的目标和研究趋势，为了达成通用多模态基础模型，主要从以下几个方面入手：
  
  **(1)统一建模（Unified Modeling）**：
  
  - •
  
    为了构建通用基础模型，需要一个可扩展且能够适用于各种下游任务的统一模型架构。这种统一化可以解锁新的模型能力，但实现各种视觉语言（VL）理解任务和区域级定位任务的统一化是一个挑战。
  
  **(2)跨域的计算机视觉（Computer Vision in the Wild）**：
  
  - •
  
    探讨了自然语言在计算机视觉任务中如何发挥更基础的作用，例如如何将图像分类、目标检测和分割等视觉识别任务视为VL问题。
  
  **(3)模型规模扩大化（Model Scaling）**：
  
  - •
  
    近年来，通过从大量文本数据训练大型Transformer模型取得了巨大成功。类似地，VLP模型也在遵循这一趋势，但与语言模型的规模相比，VLP模型的规模化还处于起步阶段，未来有很大的扩大空间。
  
  **(4)情境下的少样本学习（In-context Few-shot Learning）**：
  
  - •
  
    探索训练模型以便它能够快速适应不同的下游任务，仅需少量示例。
  
  **(5)对不同任务的高效适应（Efficient Adaptation）**：
  
  - •
  
    随着VL模型规模的迅速增长，开发高效适应下游任务的方法变得日益重要。
  
  **(6)知识（Knowledge）**：
  
  - •
  
    大型基础模型在模型权重中封装了大量关于视觉世界的多模态知识。探索使用外部知识增强预训练模型。
  
  **(7)鲁棒性（Robustness）**：
  
  - •
  
    在标准和成熟的基准测试中评估模型，同时也需要进行精心设计的鲁棒性评估。